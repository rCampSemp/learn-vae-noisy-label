{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  \n",
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603728993639/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from mlflow import pytorch\n",
    "from pprint import pformat\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from Deterministic_Loss import deterministic_noisy_label_loss\n",
    "from Utilis import seg_score, CustomDataset_LIDC, LIDC_collate\n",
    "from Deterministic_CM import UNet_DCM\n",
    "from adamW import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(options, model, device, train_loader, optimizer, epoch, num_epochs, ramp_up):\n",
    "    model.train()\n",
    "    num_batches = len(train_loader)\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    for batch_idx, (images, true_image, annots, imagename) in enumerate(train_loader):\n",
    "        # zero graidents before each iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # cast numpy data into tensor float\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        true_image = true_image.to(device=device, dtype=torch.float32)\n",
    "        annots = annots.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        outputs_logits, stochastic_cm = model(images)\n",
    "\n",
    "        # calculate loss:\n",
    "        loss = deterministic_noisy_label_loss(outputs_logits, stochastic_cm, annots, epoch, num_epochs, 'lidc', ramp_up)\n",
    "        # calculate the gradients:\n",
    "        loss.backward()\n",
    "        # update weights in model:\n",
    "        optimizer.step()\n",
    "\n",
    "        # Now outputs_logits is the noisy seg:\n",
    "        b_, c_, h_, w_ = outputs_logits.size() # b: batch size, c: \n",
    "        # pred_norm_prob_noisy = nn.Softmax(dim=1)(outputs_logits)\n",
    "        pred_noisy = outputs_logits.view(b_, c_, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_, 1)\n",
    "        anti_corrpution_cm = stochastic_cm.view(b_, c_ ** 2, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_ * c_).view(b_ * h_ * w_, c_, c_)\n",
    "        anti_corrpution_cm = torch.softmax(anti_corrpution_cm, dim=1)\n",
    "        # compute the estimated annotator's segmentation probability\n",
    "        outputs_clean = torch.bmm(anti_corrpution_cm, pred_noisy).view(b_ * h_ * w_, c_)\n",
    "        # reshape \n",
    "        outputs_clean = outputs_clean.view(b_, h_ * w_, c_).permute(0, 2, 1).contiguous().view(b_, c_, h_, w_)\n",
    "\n",
    "        _, train_output = torch.max(outputs_clean, dim=1)\n",
    "        train_iou = seg_score(true_image.cpu().detach().numpy(), train_output.cpu().detach().numpy())\n",
    "        running_loss += loss\n",
    "        running_iou += train_iou\n",
    "\n",
    "        if (batch_idx + 1) == 1:\n",
    "            print('Step [{}/{}], '\n",
    "                'Train loss: {:.4f}, '\n",
    "                'Train dice: {:.4f},'.format(epoch + 1, num_epochs,\n",
    "                                                            running_loss / (batch_idx + 1),\n",
    "                                                            running_iou / (batch_idx + 1)))\n",
    "    avg_loss = running_loss / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    return avg_iou, float(avg_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, test_loader, epoch, num_epochs, ramp_up):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_dice = 0\n",
    "    # test_dice_all = []\n",
    "    num_batches = len(test_loader)\n",
    "    #\n",
    "    for i, (v_images, v_true_image, v_annots, v_imagename) in enumerate(test_loader):\n",
    "        #\n",
    "        v_images = v_images.to(device=device, dtype=torch.float32)\n",
    "        v_outputs_logits, cms = model(v_images)\n",
    "        b, c, h, w = v_outputs_logits.size()\n",
    "        v_outputs_logits = nn.Softmax(dim=1)(v_outputs_logits)\n",
    "        # cms = model2(v_images)\n",
    "        #\n",
    "        _, v_output = torch.max(v_outputs_logits, dim=1)\n",
    "        #\n",
    "        v_dice_ = seg_score(v_true_image, v_output.cpu().detach().numpy())\n",
    "        #\n",
    "        # epoch_noisy_labels = [v_true_image.cpu().detach().numpy(), v_labels_under.cpu().detach().numpy(), v_labels_wrong.cpu().detach().numpy(), v_labels_true.cpu().detach().numpy(), v_labels_good.cpu().detach().numpy()]\n",
    "        # v_ged = generalized_energy_distance(epoch_noisy_labels, v_outputs_noisy, class_no)\n",
    "        test_dice += v_dice_\n",
    "        # test_dice_all.append(test_dice)\n",
    "        #\n",
    "        loss = deterministic_noisy_label_loss(v_outputs_logits, cms, v_annots.cpu().detach(), epoch, num_epochs, data='lidc', ramp_up=ramp_up)\n",
    "        test_loss += loss\n",
    "        \n",
    "    # print(i)\n",
    "    # print(test_dice)\n",
    "    # print(test_dice / (i + 1))\n",
    "    #\n",
    "    avg_dice = test_dice / num_batches\n",
    "    avg_loss = test_loss / num_batches\n",
    "    return avg_dice, float(avg_loss.cpu().detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the local path of the active mlflow run to save artifacts to\n",
    "def get_artifact_path(active_run):\n",
    "    parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "    artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "    return artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain hyperparameters for this trial\n",
    "def suggest_hyperparameters(trial):\n",
    "    # Obtain the learning rate on a logarithmic scale\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    # Obtain ramp-up value\n",
    "    ramp_up = trial.suggest_float(\"ramp_up\", 0.0, 1.0, step=0.1)\n",
    "    # Obtain the AdamW weight decay\n",
    "    weight_d = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "  \n",
    "    print(f\"Suggested hyperparameters: \\n{pformat(trial.params)}\")\n",
    "    # Log the obtained trial parameters using mlflow\n",
    "    mlflow.log_params(trial.params)\n",
    "    return lr, ramp_up, weight_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(train_batchsize, validate_batchsize, data_path):\n",
    "    #\n",
    "    train_path = data_path + '/train'\n",
    "    validate_path = data_path + '/validate'\n",
    "    #\n",
    "    train_dataset = CustomDataset_LIDC(dataset_location=train_path, augmentation=True)\n",
    "    #\n",
    "    validate_dataset = CustomDataset_LIDC(dataset_location=validate_path, augmentation=False)\n",
    "    #\n",
    "    trainloader = data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=4, drop_last=True, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    validateloader = data.DataLoader(validate_dataset, batch_size=validate_batchsize, shuffle=False, num_workers=4, drop_last=False, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    return trainloader, validateloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, experiment, options=None):\n",
    "    # Initialize the best validation loss, which is the value to be minimized by the network\n",
    "    best_val_loss = 0\n",
    "    \n",
    "    # Start mlflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "        # Use mlflow to log experiment options\n",
    "        mlflow.log_params(options)\n",
    "        batch_size = 128\n",
    "        num_epochs = 80\n",
    "\n",
    "        # Get hyperparameter suggestions created by optuna\n",
    "        lr, ramp_up, weight_d = suggest_hyperparameters(trial)\n",
    "        \n",
    "        print(f\"\\n**************************\")\n",
    "\n",
    "        active_run = mlflow.active_run()\n",
    "        print(f\"Starting run {active_run.info.run_id} and trial {trial.number}\")\n",
    "\n",
    "        # Parse the active mlflow run's artifact_uri and convert it into a system path\n",
    "        parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "        artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "        print(f\"Artifact path for this run: {artifact_path}\")\n",
    "        \n",
    "        # Use CUDA if GPU is available, else CPU\n",
    "        use_cuda = options[\"use_cuda\"] and torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        # Log mlflow device parameter\n",
    "        mlflow.log_param(\"device\", device)\n",
    "\n",
    "        # Obtain the MNIST train and validation loaders using a helper function\n",
    "        train_loader, val_loader = getDataLoaders(train_batchsize=batch_size, validate_batchsize=batch_size, data_path='./LIDC_examples')\n",
    "        \n",
    "        # Initialize network\n",
    "        model_seg = UNet_DCM(in_ch=1,\n",
    "                            resolution=64,\n",
    "                            width=36,\n",
    "                            depth=3,\n",
    "                            latent=512,\n",
    "                            class_no=2,\n",
    "                            norm='in').to(device)\n",
    "\n",
    "        # Pick an optimizer based on optuna's parameter suggestion\n",
    "        # if optimizer_name == \"Adam\":\n",
    "        #     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # if optimizer_name == \"Adadelta\":\n",
    "        #     optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "        # scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        optimizer = AdamW(model_seg.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_d)\n",
    "        # Network training & validation loop\n",
    "        for epoch in range(0, num_epochs):\n",
    "            avg_train_dice_loss, avg_loss = train(options, model_seg, device, train_loader, optimizer, epoch, num_epochs=num_epochs, ramp_up=ramp_up)\n",
    "            avg_val_loss, avg_val_main_loss = validate(model_seg, device, val_loader, epoch, num_epochs, ramp_up=ramp_up)\n",
    "            \n",
    "            if avg_val_loss >= best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "\n",
    "            # Report intermediate objective value.\n",
    "            trial.report(avg_val_loss, step=epoch)\n",
    "            # trial.report(avg_loss, step=epoch)\n",
    "\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Log average train and test set loss for the current epoch using mlflow\n",
    "            mlflow.log_metric(\"avg_train_dice_loss\", avg_train_dice_loss, step=epoch)\n",
    "            mlflow.log_metric(\"avg_train_main_loss\", avg_loss, step=epoch)\n",
    "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
    "            mlflow.log_metric(\"avg_val_main_loss\", avg_val_main_loss, step=epoch)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save the final network model to the current mlflow run's directory \n",
    "        if options[\"save_model\"]:\n",
    "            pytorch.save_model(model_seg, f\"{artifact_path}/LIDC_model\")\n",
    "\n",
    "    # Return the best validation loss\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    options = {\n",
    "        \"experiment_name\": \"54GB-RAM\",\n",
    "        \"use_cuda\": False,\n",
    "        \"save_model\": True\n",
    "    }\n",
    "   \n",
    "    # Create mlflow experiment if it doesn't exist already\n",
    "    experiment_name = options[\"experiment_name\"]\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Propagate logs to the root logger.\n",
    "    optuna.logging.set_verbosity(verbosity=optuna.logging.INFO)\n",
    "\n",
    "    # Create the optuna study which shares the experiment name\n",
    "    study = optuna.create_study(study_name=experiment_name, direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, experiment, options), n_trials=30)\n",
    "\n",
    "    # Filter optuna trials by state\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Trial number: \", trial.number)\n",
    "    print(\"  Loss (trial value): \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('Lung_vessel_segmentation_HRCT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60c0b920f7ac444c368da2d69263238fbf8c6dd0b23b03ce47e2fc406898cab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
