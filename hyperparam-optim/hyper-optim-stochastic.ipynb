{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  \n",
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603728993639/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from mlflow import pytorch\n",
    "from pprint import pformat\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from Stochastic_Loss import stochastic_noisy_label_loss\n",
    "from Utilis import seg_score, CustomDataset_LIDC, LIDC_collate\n",
    "from Stochastic_CM import UNet_SCM\n",
    "from adamW import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPred(pred_seg_logits, cm):\n",
    "    # Now outputs_logits is the noisy seg:\n",
    "    b_, c_, h_, w_ = pred_seg_logits.size()\n",
    "\n",
    "    pred_noisy = pred_seg_logits.view(b_, c_, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_, 1)\n",
    "\n",
    "    # reshape cm: [ b , c , c , h , w]= > [ b∗h∗w, c , c ]\n",
    "    anti_corrpution_cm = cm.view(b_, c_ ** 2, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_ * c_).view(b_ * h_ * w_, c_, c_)\n",
    "    anti_corrpution_cm = torch.softmax(anti_corrpution_cm, dim=1)\n",
    "    # compute estimated annotators noisy seg proability\n",
    "    outputs_clean = torch.bmm(anti_corrpution_cm, pred_noisy).view(b_ * h_ * w_, c_)\n",
    "    # reshape: [ b∗h∗w, c , 1 ] => [ b , c , h , w ]\n",
    "    outputs_clean = outputs_clean.view(b_, h_ * w_, c_).permute(0, 2, 1).contiguous().view(b_, c_, h_, w_)\n",
    "\n",
    "    # probability -> binary mask\n",
    "    _, train_output = torch.max(outputs_clean, dim=1)\n",
    "\n",
    "    return train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(options, model, device, train_loader, optimizer, epoch, num_epochs, ramp_up):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_kld_loss = 0\n",
    "    running_iou = 0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_idx, (images, true_image, annots, imagename) in enumerate(train_loader):\n",
    "        # zero graidents before each iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # cast numpy data into tensor float\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        true_image = true_image.to(device=device, dtype=torch.float32)\n",
    "        annots = annots.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        # model has two outputs:\n",
    "        # first one is the probability map for true ground truth\n",
    "        # second one is a list collection of probability maps for different noisy ground truths\n",
    "        outputs_logits, sampled_cm, mean, logvar = model(images)\n",
    "        # outputs = 0.9*outputs + 0.1*outputs_logits\n",
    "        \n",
    "        # calculate loss:\n",
    "        seg_loss, kldloss = stochastic_noisy_label_loss(outputs_logits, sampled_cm, mean, logvar, annots, epoch, num_epochs, data='lidc', ramp_up=ramp_up)\n",
    "        # print(images.size())\n",
    "        loss = seg_loss + kldloss\n",
    "        # calculate the gradients:\n",
    "        # loss = lossss(model, sampled_outputs_logits, stochastic_cm, mean, logvar, label, epoch, num_epochs, ramp_up, alpha)\n",
    "        loss.backward()\n",
    "        # update weights in model:\n",
    "        optimizer.step()\n",
    "\n",
    "        train_output = calcPred(outputs_logits, sampled_cm)\n",
    "\n",
    "        train_iou = seg_score(true_image.detach().numpy(), train_output.detach().numpy())\n",
    "        running_loss += seg_loss\n",
    "        running_kld_loss += kldloss\n",
    "        running_iou += train_iou\n",
    "\n",
    "        if (batch_idx + 1) == 1:\n",
    "            print(\n",
    "                'Step [{}/{}], '\n",
    "                'Train loss: {:.4f}, '\n",
    "                'Train kld: {:.4f},'\n",
    "                'Train dice: {:.4f},'.format(epoch + 1, num_epochs,\n",
    "                                                            running_loss,\n",
    "                                                            running_kld_loss,\n",
    "                                                            running_iou))\n",
    "    av_loss = running_loss / num_batches\n",
    "    av_kld = running_kld_loss / num_batches\n",
    "    av_dice = running_iou / num_batches\n",
    "\n",
    "    train_loss_main = av_kld + av_loss\n",
    "\n",
    "    return float(train_loss_main.cpu().detach().numpy()), av_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader, model, device, epoch, num_epochs, ramp_up):\n",
    "    model.eval()\n",
    "\n",
    "    running_dice = 0\n",
    "    running_kld_loss = 0\n",
    "    running_seg_loss = 0\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, true_image, annots, imagename) in enumerate(data_loader):\n",
    "            # cast numpy data into tensor float\n",
    "            images = images.to(device=device, dtype=torch.float32)\n",
    "            true_image = true_image.to(device=device, dtype=torch.float32)\n",
    "            annots = annots.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            # model has two outputs:\n",
    "            # first one is the probability map for true ground truth\n",
    "            # second one is a list collection of probability maps for different noisy ground truths\n",
    "            outputs_logits, sampled_cm, mean, logvar = model(images)\n",
    "            # outputs = 0.9*outputs + 0.1*outputs_logits\n",
    "            \n",
    "            # calculate loss:\n",
    "            seg_loss, kldloss = stochastic_noisy_label_loss(outputs_logits, sampled_cm, mean, logvar, annots, epoch, num_epochs, data='lidc', ramp_up=ramp_up)\n",
    "\n",
    "            train_output = calcPred(outputs_logits, sampled_cm)\n",
    "            train_iou = seg_score(true_image, train_output)\n",
    "            \n",
    "            running_seg_loss += seg_loss\n",
    "            running_kld_loss += kldloss\n",
    "            running_dice += train_iou\n",
    "    \n",
    "    av_dice = running_dice / num_batches\n",
    "    av_kld = running_kld_loss / num_batches\n",
    "    av_seg_loss = running_seg_loss / num_batches\n",
    "    val_loss_main = av_kld + av_seg_loss\n",
    "\n",
    "    return av_dice, float(val_loss_main.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the local path of the active mlflow run to save artifacts to\n",
    "def get_artifact_path(active_run):\n",
    "    parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "    artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "    return artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain hyperparameters for this trial\n",
    "def suggest_hyperparameters(trial):\n",
    "    # Obtain the learning rate on a logarithmic scale\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    # Obtain ramp-up value\n",
    "    ramp_up = trial.suggest_float(\"ramp_up\", 0.0, 1.0, step=0.1)\n",
    "    # Obtain the AdamW weight decay\n",
    "    weight_d = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    # Obtain the width of model\n",
    "    width = trial.suggest_int(\"width\", 8, 44, step=4)\n",
    "    # Obtain depth of model\n",
    "    depth = trial.suggest_int(\"depth\", 1, 6, step=2)\n",
    "\n",
    "    print(f\"Suggested hyperparameters: \\n{pformat(trial.params)}\")\n",
    "    # Log the obtained trial parameters using mlflow\n",
    "    mlflow.log_params(trial.params)\n",
    "    return lr, ramp_up, weight_d, width, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(train_batchsize, validate_batchsize, data_path):\n",
    "    #\n",
    "    train_path = data_path + '/train'\n",
    "    validate_path = data_path + '/validate'\n",
    "    #\n",
    "    train_dataset = CustomDataset_LIDC(dataset_location=train_path, augmentation=True)\n",
    "    #\n",
    "    validate_dataset = CustomDataset_LIDC(dataset_location=validate_path, augmentation=False)\n",
    "    #\n",
    "    trainloader = data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=4, drop_last=True, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    validateloader = data.DataLoader(validate_dataset, batch_size=validate_batchsize, shuffle=False, num_workers=4, drop_last=False, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    return trainloader, validateloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, experiment, options=None):\n",
    "    # Initialize the best validation loss, which is the value to be minimized by the network\n",
    "    best_val_loss = 0\n",
    "    \n",
    "    # Start mlflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "        # Use mlflow to log experiment options\n",
    "        mlflow.log_params(options)\n",
    "        batch_size = 128\n",
    "        num_epochs = 80\n",
    "        # Get hyperparameter suggestions created by optuna\n",
    "        lr, ramp_up, weight_d, width, depth = suggest_hyperparameters(trial)\n",
    "        \n",
    "        print(f\"\\n**************************\")\n",
    "\n",
    "        active_run = mlflow.active_run()\n",
    "        print(f\"Starting run {active_run.info.run_id} and trial {trial.number}\")\n",
    "\n",
    "        # Parse the active mlflow run's artifact_uri and convert it into a system path\n",
    "        parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "        artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "        print(f\"Artifact path for this run: {artifact_path}\")\n",
    "        \n",
    "        # Use CUDA if GPU is available, else CPU\n",
    "        use_cuda = options[\"use_cuda\"] and torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        # Log mlflow device parameter\n",
    "        mlflow.log_param(\"device\", device)\n",
    "\n",
    "        # Obtain the MNIST train and validation loaders using a helper function\n",
    "        train_loader, val_loader = getDataLoaders(train_batchsize=batch_size, validate_batchsize=batch_size, data_path='./LIDC_examples')\n",
    "        \n",
    "        # Initialize network\n",
    "        model = UNet_SCM(in_ch=1,\n",
    "                            resolution=64,\n",
    "                            width=width,\n",
    "                            depth=depth,\n",
    "                            latent=512,\n",
    "                            class_no=2,\n",
    "                            norm='in').to(device)\n",
    "\n",
    "        # Pick an optimizer based on optuna's parameter suggestion\n",
    "        # if optimizer_name == \"Adam\":\n",
    "        #     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # if optimizer_name == \"Adadelta\":\n",
    "        #     optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "        # scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        optimizer = AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_d)\n",
    "        \n",
    "        # Network training & validation loop\n",
    "        for epoch in range(0, num_epochs):\n",
    "            train_loss, train_dice = train(options, model, device, train_loader, optimizer, epoch, num_epochs=num_epochs, ramp_up=ramp_up)\n",
    "            val_dice, val_loss = validate(val_loader, model, device, epoch, num_epochs, ramp_up=ramp_up)\n",
    "            \n",
    "            if val_dice >= best_val_loss:\n",
    "                best_val_loss = val_dice\n",
    "\n",
    "            # Report intermediate objective value.\n",
    "            trial.report(val_loss, step=epoch)\n",
    "            # trial.report(avg_loss, step=epoch)\n",
    "\n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Log average train and test set loss for the current epoch using mlflow\n",
    "            mlflow.log_metric(\"train_dice\", train_dice, step=epoch)\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "            mlflow.log_metric(\"val_dice\", val_dice, step=epoch)\n",
    "            mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save the final network model to the current mlflow run's directory \n",
    "        if options[\"save_model\"]:\n",
    "            pytorch.save_model(model, f\"{artifact_path}/Sto_LIDC_model\")\n",
    "\n",
    "    # Return the best validation loss\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    options = {\n",
    "        \"experiment_name\": \"Stochastic-LIDC-dice\",\n",
    "        \"use_cuda\": False,\n",
    "        \"save_model\": True\n",
    "    }\n",
    "   \n",
    "    # Create mlflow experiment if it doesn't exist already\n",
    "    experiment_name = options[\"experiment_name\"]\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Propagate logs to the root logger.\n",
    "    optuna.logging.set_verbosity(verbosity=optuna.logging.INFO)\n",
    "\n",
    "    # Create the optuna study which shares the experiment name\n",
    "    study = optuna.create_study(study_name=experiment_name, direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, experiment, options), n_trials=200)\n",
    "\n",
    "    # Filter optuna trials by state\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Trial number: \", trial.number)\n",
    "    print(\"  Loss (trial value): \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('Lung_vessel_segmentation_HRCT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60c0b920f7ac444c368da2d69263238fbf8c6dd0b23b03ce47e2fc406898cab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
