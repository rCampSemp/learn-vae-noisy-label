{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/rhys/miniconda3/envs/Lung_vessel_segmentation_HRCT/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603728993639/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from mlflow import pytorch\n",
    "from pprint import pformat\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from Deterministic_LIDC_Loss import deterministic_noisy_label_loss\n",
    "from Utilis import seg_score, CustomDataset_LIDC, LIDC_collate\n",
    "from Deterministic_LIDC_CM import UNet_DCM\n",
    "from adamW import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(options, model, device, train_loader, optimizer, epoch, num_epochs, ramp_up):\n",
    "    model.train()\n",
    "    num_batches = len(train_loader)\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    for batch_idx, (images, true_image, annots, imagename) in enumerate(train_loader):\n",
    "        # zero graidents before each iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # cast numpy data into tensor float\n",
    "        images = images.to(device=device, dtype=torch.float32)\n",
    "        true_image = true_image.to(device=device, dtype=torch.float32)\n",
    "        annots = annots.to(device=device, dtype=torch.float32)\n",
    "\n",
    "        outputs_logits, stochastic_cm = model(images)\n",
    "\n",
    "        # calculate loss:\n",
    "        loss = deterministic_noisy_label_loss(outputs_logits, stochastic_cm, annots, epoch, num_epochs, ramp_up)\n",
    "        # calculate the gradients:\n",
    "        loss.backward()\n",
    "        # update weights in model:\n",
    "        optimizer.step()\n",
    "\n",
    "        # Now outputs_logits is the noisy seg:\n",
    "        b_, c_, h_, w_ = outputs_logits.size() # b: batch size, c: \n",
    "        # pred_norm_prob_noisy = nn.Softmax(dim=1)(outputs_logits)\n",
    "        pred_noisy = outputs_logits.view(b_, c_, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_, 1)\n",
    "        anti_corrpution_cm = stochastic_cm.view(b_, c_ ** 2, h_ * w_).permute(0, 2, 1).contiguous().view(b_ * h_ * w_, c_ * c_).view(b_ * h_ * w_, c_, c_)\n",
    "        anti_corrpution_cm = torch.softmax(anti_corrpution_cm, dim=1)\n",
    "        # compute the estimated annotator's segmentation probability\n",
    "        outputs_clean = torch.bmm(anti_corrpution_cm, pred_noisy).view(b_ * h_ * w_, c_)\n",
    "        # reshape \n",
    "        outputs_clean = outputs_clean.view(b_, h_ * w_, c_).permute(0, 2, 1).contiguous().view(b_, c_, h_, w_)\n",
    "\n",
    "        _, train_output = torch.max(outputs_clean, dim=1)\n",
    "        train_iou = seg_score(true_image.cpu().detach().numpy(), train_output.cpu().detach().numpy())\n",
    "        running_loss += loss\n",
    "        running_iou += train_iou\n",
    "\n",
    "        if (batch_idx + 1) == 1:\n",
    "            print('Step [{}/{}], '\n",
    "                'Train loss: {:.4f}, '\n",
    "                'Train dice: {:.4f},'.format(epoch + 1, num_epochs,\n",
    "                                                            running_loss / (batch_idx + 1),\n",
    "                                                            running_iou / (batch_idx + 1)))\n",
    "    # avg_loss = running_loss / num_batches\n",
    "    avg_iou = running_iou / num_batches\n",
    "    return avg_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_dice = 0\n",
    "    test_dice_all = []\n",
    "    num_batches = len(test_loader)\n",
    "    #\n",
    "    for i, (v_images, v_true_image, v_annots, v_imagename) in enumerate(test_loader):\n",
    "        #\n",
    "        v_images = v_images.to(device=device, dtype=torch.float32)\n",
    "        v_outputs_logits, cms = model(v_images)\n",
    "        b, c, h, w = v_outputs_logits.size()\n",
    "        v_outputs_logits = nn.Softmax(dim=1)(v_outputs_logits)\n",
    "        # cms = model2(v_images)\n",
    "        #\n",
    "        _, v_output = torch.max(v_outputs_logits, dim=1)\n",
    "        #\n",
    "        v_dice_ = seg_score(v_true_image, v_output.cpu().detach().numpy())\n",
    "        #\n",
    "        # epoch_noisy_labels = [v_true_image.cpu().detach().numpy(), v_labels_under.cpu().detach().numpy(), v_labels_wrong.cpu().detach().numpy(), v_labels_true.cpu().detach().numpy(), v_labels_good.cpu().detach().numpy()]\n",
    "        # v_ged = generalized_energy_distance(epoch_noisy_labels, v_outputs_noisy, class_no)\n",
    "        test_dice += v_dice_\n",
    "        test_dice_all.append(test_dice)\n",
    "        #\n",
    "    # print(i)\n",
    "    # print(test_dice)\n",
    "    # print(test_dice / (i + 1))\n",
    "    #\n",
    "    avg_dice = test_dice / num_batches\n",
    "    return avg_dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the local path of the active mlflow run to save artifacts to\n",
    "def get_artifact_path(active_run):\n",
    "    parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "    artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "    return artifact_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain hyperparameters for this trial\n",
    "def suggest_hyperparameters(trial):\n",
    "    # Obtain the learning rate on a logarithmic scale\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    # Obtain ramp-up value\n",
    "    ramp_up = trial.suggest_float(\"ramp_up\", 0.0, 0.9, step=0.1)\n",
    "    # Obtain the number of epochs\n",
    "    num_epochs = trial.suggest_int(\"epochs\", 1, 5, step=1)\n",
    "    # Obtain the batch size (as power of 2)\n",
    "    batch_size = 2 ** trial.suggest_int(\"batch_size_power\", 1, 2, step=1)\n",
    "    # Obtain the AdamW weight decay\n",
    "    weight_d = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    # Obtain the optimizer to use by name\n",
    "    # optimizer_name = trial.suggest_categorical(\"optimizer_name\", [\"AdamW\", \"Adadelta\"])\n",
    "\n",
    "    print(f\"Suggested hyperparameters: \\n{pformat(trial.params)}\")\n",
    "    # Log the obtained trial parameters using mlflow\n",
    "    mlflow.log_params(trial.params)\n",
    "    return lr, ramp_up, num_epochs, batch_size, weight_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLoaders(train_batchsize, validate_batchsize, data_path):\n",
    "    #\n",
    "    train_path = data_path + '/train'\n",
    "    validate_path = data_path + '/validate'\n",
    "    # test_path = data_path + '/test'\n",
    "    #\n",
    "    train_dataset = CustomDataset_LIDC(dataset_location=train_path, augmentation=True)\n",
    "    #\n",
    "    validate_dataset = CustomDataset_LIDC(dataset_location=validate_path, augmentation=False)\n",
    "    #\n",
    "    trainloader = data.DataLoader(train_dataset, batch_size=train_batchsize, shuffle=True, num_workers=5, drop_last=True, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    validateloader = data.DataLoader(validate_dataset, batch_size=validate_batchsize, shuffle=False, num_workers=validate_batchsize, drop_last=False, collate_fn=LIDC_collate)\n",
    "    #\n",
    "    return trainloader, validateloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, experiment, options=None):\n",
    "    # Initialize the best validation loss, which is the value to be minimized by the network\n",
    "    best_val_loss = float('Inf')\n",
    "    \n",
    "    # Start mlflow run\n",
    "    with mlflow.start_run(experiment_id=experiment.experiment_id):\n",
    "        # Use mlflow to log experiment options\n",
    "        mlflow.log_params(options)\n",
    "        \n",
    "        # Get hyperparameter suggestions created by optuna\n",
    "        lr, ramp_up, num_epochs, batch_size, weight_d = suggest_hyperparameters(trial)\n",
    "        \n",
    "        print(f\"\\n**************************\")\n",
    "\n",
    "        active_run = mlflow.active_run()\n",
    "        print(f\"Starting run {active_run.info.run_id} and trial {trial.number}\")\n",
    "\n",
    "        # Parse the active mlflow run's artifact_uri and convert it into a system path\n",
    "        parsed_uri = urlparse(active_run.info.artifact_uri)\n",
    "        artifact_path = os.path.abspath(os.path.join(parsed_uri.netloc, parsed_uri.path))\n",
    "        print(f\"Artifact path for this run: {artifact_path}\")\n",
    "        \n",
    "        # Use CUDA if GPU is available, else CPU\n",
    "        use_cuda = options[\"use_cuda\"] and torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "        # Log mlflow device parameter\n",
    "        mlflow.log_param(\"device\", device)\n",
    "\n",
    "        # Obtain the MNIST train and validation loaders using a helper function\n",
    "        train_loader, val_loader = getDataLoaders(train_batchsize=batch_size, validate_batchsize=batch_size, data_path='./LIDC_examples')\n",
    "        \n",
    "        # Initialize network\n",
    "        model_seg = UNet_DCM(in_ch=1,\n",
    "                            resolution=64,\n",
    "                            width=24,\n",
    "                            depth=3,\n",
    "                            latent=512,\n",
    "                            class_no=2,\n",
    "                            norm='in').to(device)\n",
    "\n",
    "        # Pick an optimizer based on optuna's parameter suggestion\n",
    "        # if optimizer_name == \"Adam\":\n",
    "        #     optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        # if optimizer_name == \"Adadelta\":\n",
    "        #     optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "        # scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "        optimizer = AdamW(model_seg.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_d)\n",
    "\n",
    "        # Network training & validation loop\n",
    "        for epoch in range(0, num_epochs):\n",
    "            avg_train_dice_loss = train(options, model_seg, device, train_loader, optimizer, epoch, num_epochs=num_epochs, ramp_up=ramp_up)\n",
    "            avg_val_loss = validate(model_seg, device, val_loader)\n",
    "            \n",
    "            if avg_val_loss <= best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "            \n",
    "            # Report intermediate objective value.\n",
    "            trial.report(avg_val_loss, step=epoch)\n",
    "            \n",
    "            # Handle pruning based on the intermediate value.\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # Log average train and test set loss for the current epoch using mlflow\n",
    "            mlflow.log_metric(\"avg_train_dice_loss\", avg_train_dice_loss, step=epoch)\n",
    "            # mlflow.log_metric(\"avg_train_main_loss\", avg_train_main_loss, step=epoch)\n",
    "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Save the final network model to the current mlflow run's directory \n",
    "        if options[\"save_model\"]:\n",
    "            pytorch.save_model(model_seg, f\"{artifact_path}/LIDC_model\")\n",
    "\n",
    "    # Return the best validation loss\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    options = {\n",
    "        \"experiment_name\": \"MSc-pytorch-optuna-1\",\n",
    "        \"use_cuda\": False,\n",
    "        \"save_model\": True\n",
    "    }\n",
    "   \n",
    "    # Create mlflow experiment if it doesn't exist already\n",
    "    experiment_name = options[\"experiment_name\"]\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Propagate logs to the root logger.\n",
    "    optuna.logging.set_verbosity(verbosity=optuna.logging.INFO)\n",
    "\n",
    "    # Create the optuna study which shares the experiment name\n",
    "    study = optuna.create_study(study_name=experiment_name, direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, experiment, options), n_trials=3)\n",
    "\n",
    "    # Filter optuna trials by state\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"\\n++++++++++++++++++++++++++++++++++\\n\")\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Trial number: \", trial.number)\n",
    "    print(\"  Loss (trial value): \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-29 13:19:21,808]\u001b[0m A new study created in memory with name: MSc-pytorch-optuna-1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested hyperparameters: \n",
      "{'batch_size_power': 2,\n",
      " 'epochs': 3,\n",
      " 'lr': 9.051488305350064e-05,\n",
      " 'ramp_up': 0.1,\n",
      " 'weight_decay': 0.00026532675212743236}\n",
      "\n",
      "**************************\n",
      "Starting run 2f2f617c1d984c9084786fd39cf827aa and trial 0\n",
      "Artifact path for this run: /home/rhys/Documents/MScProj/NoisyLabelsMe/mlruns/1/2f2f617c1d984c9084786fd39cf827aa/artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rhys/Documents/MScProj/NoisyLabelsMe/adamW.py:100: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1603728993639/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1/3], Train loss: 1.5489, Train dice: 0.1144,\n",
      "Step [2/3], Train loss: 0.9085, Train dice: 0.1381,\n",
      "Step [3/3], Train loss: 0.6244, Train dice: 0.4781,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-29 13:22:45,691]\u001b[0m Trial 0 finished with value: 0.2591642250983017 and parameters: {'lr': 9.051488305350064e-05, 'ramp_up': 0.1, 'epochs': 3, 'batch_size_power': 2, 'weight_decay': 0.00026532675212743236}. Best is trial 0 with value: 0.2591642250983017.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested hyperparameters: \n",
      "{'batch_size_power': 2,\n",
      " 'epochs': 4,\n",
      " 'lr': 3.8969827248638676e-05,\n",
      " 'ramp_up': 0.5,\n",
      " 'weight_decay': 2.8445561562932273e-05}\n",
      "\n",
      "**************************\n",
      "Starting run f705debd041d4107887c90d49009d204 and trial 1\n",
      "Artifact path for this run: /home/rhys/Documents/MScProj/NoisyLabelsMe/mlruns/1/f705debd041d4107887c90d49009d204/artifacts\n",
      "Step [1/4], Train loss: 1.3869, Train dice: 0.0877,\n",
      "Step [2/4], Train loss: 0.8007, Train dice: 0.2863,\n",
      "Step [3/4], Train loss: 0.5341, Train dice: 0.7527,\n",
      "Step [4/4], Train loss: 0.5461, Train dice: 0.4411,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-29 13:27:30,855]\u001b[0m Trial 1 finished with value: 0.4554908838201267 and parameters: {'lr': 3.8969827248638676e-05, 'ramp_up': 0.5, 'epochs': 4, 'batch_size_power': 2, 'weight_decay': 2.8445561562932273e-05}. Best is trial 1 with value: 0.4554908838201267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested hyperparameters: \n",
      "{'batch_size_power': 1,\n",
      " 'epochs': 2,\n",
      " 'lr': 0.00020035295334201869,\n",
      " 'ramp_up': 0.7000000000000001,\n",
      " 'weight_decay': 0.00010504017834403652}\n",
      "\n",
      "**************************\n",
      "Starting run 83ca8f498dd84bd486285765d0735646 and trial 2\n",
      "Artifact path for this run: /home/rhys/Documents/MScProj/NoisyLabelsMe/mlruns/1/83ca8f498dd84bd486285765d0735646/artifacts\n",
      "Step [1/2], Train loss: 1.3867, Train dice: 0.1437,\n",
      "Step [2/2], Train loss: 1.9248, Train dice: 0.0831,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-07-29 13:31:20,844]\u001b[0m Trial 2 finished with value: 0.20562248605933656 and parameters: {'lr': 0.00020035295334201869, 'ramp_up': 0.7000000000000001, 'epochs': 2, 'batch_size_power': 1, 'weight_decay': 0.00010504017834403652}. Best is trial 1 with value: 0.4554908838201267.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "++++++++++++++++++++++++++++++++++\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  3\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  3\n",
      "Best trial:\n",
      "  Trial number:  1\n",
      "  Loss (trial value):  0.4554908838201267\n",
      "  Params: \n",
      "    lr: 3.8969827248638676e-05\n",
      "    ramp_up: 0.5\n",
      "    epochs: 4\n",
      "    batch_size_power: 2\n",
      "    weight_decay: 2.8445561562932273e-05\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 ('Lung_vessel_segmentation_HRCT')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60c0b920f7ac444c368da2d69263238fbf8c6dd0b23b03ce47e2fc406898cab8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
